{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df6c2ac-27e7-4c56-a411-9523b7172d03",
   "metadata": {},
   "source": [
    "# Task Evaluations for OLMo and other LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b290c-3b91-4815-8e09-994bc5a58a89",
   "metadata": {},
   "source": [
    "This notebook shows how to run LM task evaluations in Beaker against HF and OLMo models, using tasks defined in catwalk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d3a62-ef83-46d9-9c22-0f4d090dd455",
   "metadata": {},
   "source": [
    "Personal settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "933451d4-d1be-467b-9ba6-085a6a764469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directory for the local repository where gantry will be run from\n",
    "REPO_DIR = \"/Users/oyvindt/gitroot/catwalk\"\n",
    "\n",
    "# Where to cache downloaded evaluations\n",
    "CACHE_DIR = \"/Users/oyvindt/evals/olmo/beaker-result-cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9a379-b1b2-42a6-9619-df6a6acfd77b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7e6e6-d5ea-450c-be6d-bf6b37f4b0ba",
   "metadata": {},
   "source": [
    "Setup instructions:\n",
    "   * Clone latest catwalk repo from https://github.com/OyvindTafjord/catwalk \n",
    "   * Install [beaker-gantry](https://github.com/allenai/beaker-gantry) and [beaker-py](https://github.com/allenai/beaker-py) (`pip install beaker-gantry` and `pip install beaker-py`)\n",
    "   * See [this document](https://docs.google.com/document/d/1HahVawRR2Nf_J_B5Adsxierp4HK01tV8NR9o6NFUgMo/edit?usp=sharing) for how to run local catwalk evaluations (to vet new tasks, etc), otherwise catwalk dependencies do not need to be installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327e72c-cae4-43c2-953c-025a2fcc6498",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Code (run this section first)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31733605-8fcd-413a-8405-5e70c07f576d",
   "metadata": {},
   "source": [
    "This is just some rough example code to help streamline beaker experiment management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "5c8bd2f3-7eeb-4886-b55a-e1675465da17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from beaker import Beaker\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "def run_gantry(exp_name, template, args, description=None, overrides=None):\n",
    "    current_dir = os.getcwd()\n",
    "    command = template\n",
    "    gantry_args = \" --name \"+exp_name\n",
    "    if description is not None:\n",
    "        description = description.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "        command += \" --description \\\"\" + description + \"\\\"\"\n",
    "    args_full = args.copy()\n",
    "    args_full['gantry_args'] = gantry_args\n",
    "    for key, value in args_full.items():\n",
    "        command = command.replace(\"$$\"+key.upper()+\"$$\",value)\n",
    "    assert \"$$\" not in command\n",
    "    if overrides is not None:\n",
    "        for key, value in overrides.items():\n",
    "            command = re.sub(f\"--{key} \\\\S+\", f\"--{key} {value}\", command)\n",
    "    os.chdir(REPO_DIR)\n",
    "    stream = os.popen(command)\n",
    "    output = stream.read()\n",
    "    os.chdir(current_dir)\n",
    "    return {\"command\": command, \"output\": output}  \n",
    "\n",
    "def make_exp_name(model_name, checkpoint=None):\n",
    "    random_hex = '%010x' % random.randrange(16**10)\n",
    "    res = \"lmeval-\" + re.sub(\".*/\", \"\", model_name)\n",
    "    if checkpoint:\n",
    "        res += \"-\" + checkpoint\n",
    "    res+= \"-\" + random_hex\n",
    "    # Make conform to valid Beaker experiment names\n",
    "    res = re.sub(\"[^-_.a-zA-Z0-9]\", \"\", res)\n",
    "    return res \n",
    "\n",
    "def get_experiment_status(exp_data):\n",
    "    exp = resolve_exp(exp_data)\n",
    "    full_status = exp.jobs[0].status\n",
    "    return {\"finalized\": f\"{full_status.finalized}\", \"exit_code\": full_status.exit_code}\n",
    "\n",
    "def resolve_exp(exp_data):\n",
    "    exp = None\n",
    "    if 'exp_id' not in exp_data:\n",
    "        exp = BEAKER.experiment.get(BEAKER.account.name + \"/\" + exp_data['exp_name'])\n",
    "        exp_data['id'] = exp.id\n",
    "    if exp is None:\n",
    "        exp = BEAKER.experiment.get(exp_data['exp_id'])\n",
    "    if 'result_id' not in exp_data:\n",
    "        exp_data['result_id'] = exp.jobs[0].result.beaker\n",
    "    return exp\n",
    "\n",
    "def get_result_path(exp_data, cache_dir=CACHE_DIR, force_download=False):\n",
    "    exp = resolve_exp(exp_data)\n",
    "    result_id = exp_data['result_id']\n",
    "    exp_dir = os.path.join(cache_dir, result_id)\n",
    "    if os.path.exists(exp_dir) and not force_download and len(os.listdir(exp_dir)) > 0:\n",
    "        return exp_dir\n",
    "    BEAKER.dataset.fetch(result_id, target=exp_dir)\n",
    "    return exp_dir\n",
    "\n",
    "def run_gantry_from_model_spec(model_spec, checkpoint, template, task_set):\n",
    "    model_name = model_spec['name']\n",
    "    exp_name = make_exp_name(model_name, checkpoint)\n",
    "    model_full = f\"lm::pretrained={model_name}\"\n",
    "    if checkpoint:\n",
    "        model_full += f\",revision={checkpoint}\"\n",
    "    print(f\"Running {exp_name}\")\n",
    "    args = {\"model\": model_full, \"task\": task_set}\n",
    "    if \"beaker_model\" in model_spec:\n",
    "        args['beaker_model'] = model_spec['beaker_model']\n",
    "    run_output = run_gantry(exp_name, template, args, overrides = model_spec.get(\"overrides\"))\n",
    "    return {\"exp_name\": exp_name, \"model\": model_name, \"checkpoint\": checkpoint, \"run_gantry\": run_output}  \n",
    "\n",
    "def load_jsonl(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        return [json.loads(line.strip()) for line in file]\n",
    "\n",
    "def load_json(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        return json.loads(file.read())\n",
    "    \n",
    "\n",
    "def save_jsonl(file_name, data):\n",
    "    with open(file_name, 'w') as file:\n",
    "        for d in data:\n",
    "            file.write(json.dumps(d))\n",
    "            file.write(\"\\n\")\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f6cc1-281f-435b-a385-e9bfb5ebcd04",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccfeb62-57b6-4ee2-a3e3-0cb16969b19d",
   "metadata": {},
   "source": [
    "Setup beaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ffaded86-5749-4860-ad1f-69bc1fd172ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BEAKER = Beaker.from_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c021ec62-0c4e-45dc-a514-e6a42207df55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oyvindt'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEAKER.account.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df0210-ec7c-4877-93d0-0796ef7c2a13",
   "metadata": {},
   "source": [
    "## Start experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8993f-c638-4513-944b-0d34cca7e68f",
   "metadata": {},
   "source": [
    "Here we'll show an example of running a few Pythia checkpoints against a set of 20 catwalk tasks in default mode (zero-shot). Note that tasks can also be defined in a task_file with individual options for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a70df0a6-a370-4a92-a647-6a68dbfd79c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TASK_SET = \"arc_challenge arc_easy boolq copa headqa_en hellaswag logiqa mathqa mrpc openbookqa piqa qnli qqp rte sciq sst wic winogrande wnli wsc\"\n",
    "MODELS_TO_RUN = [ \n",
    "    {\"name\": \"EleutherAI/pythia-6.9b\", \"checkpoints\": [\"step1000\",\"step10000\", \"step50000\", \"step140000\"], \"overrides\": {\"max_batch_tokens\": 2048*2}},\n",
    "    {\"name\": \"EleutherAI/pythia-160m\", \"checkpoints\": [\"step1000\",\"step10000\", \"step50000\", \"step140000\"]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a645176e-7d9c-4259-809a-4a8a09a9c67b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_tracker = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4a584-ebd3-4ff6-951d-afdc258c321a",
   "metadata": {},
   "source": [
    "Launch the experiments, using this command with a few variables to fill in (most of the other variables should be self-explanatory, but see [document](https://docs.google.com/document/d/1HahVawRR2Nf_J_B5Adsxierp4HK01tV8NR9o6NFUgMo/edit?usp=sharing) for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "62396819-8f74-43fb-a6f3-c85a6d642683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GANTRY_TEMPLATE_ZEROSHOT_1 = \"gantry run --gpus 1 --venv base --workspace ai2/lm-eval \\\n",
    "--cluster ai2/aristo-cirrascale --beaker-image oyvindt/OLMoEvalV4 \\\n",
    "--env 'HF_DATASETS_CACHE=/net/nfs.cirrascale/aristo/oyvindt/hf_datasets_cache' $$GANTRY_ARGS$$ \\\n",
    "-- python catwalk/run_lm_eval.py --model $$MODEL$$ --task $$TASK$$ --split validation \\\n",
    "--full_output_file /results/predictions.jsonl --metrics_file /results/metrics.json --limit 1000 \\\n",
    "--num_recorded_inputs 3 --batch_size 32 --model_max_length 2048 --max_batch_tokens 20480\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9ef849fd-0027-41bf-8ba0-ad69be82e617",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running lmeval-pythia-6.9b-step1000-9113d68b6a\n",
      "Running lmeval-pythia-6.9b-step10000-cacb9eb3c6\n",
      "Running lmeval-pythia-6.9b-step50000-715023ac71\n",
      "Running lmeval-pythia-6.9b-step140000-a8eb0a3c7c\n",
      "Running lmeval-pythia-160m-step1000-af94704947\n",
      "Running lmeval-pythia-160m-step10000-3b698b190a\n",
      "Running lmeval-pythia-160m-step50000-0784ce9ab1\n",
      "Running lmeval-pythia-160m-step140000-18fc9c1c2f\n"
     ]
    }
   ],
   "source": [
    "for model_spec in MODELS_TO_RUN:\n",
    "    for checkpoint in model_spec[\"checkpoints\"]:\n",
    "        res = run_gantry_from_model_spec(model_spec, checkpoint, GANTRY_TEMPLATE_ZEROSHOT_1, TASK_SET)\n",
    "        experiment_tracker.append(res)\n",
    "        time.sleep(0.5)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a12c871-816b-475f-80b3-a4f23e013106",
   "metadata": {},
   "source": [
    "Check status of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ad803c82-8cce-4078-ac08-cb0e5de55a00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lmeval-pythia-6.9b-step1000-9113d68b6a', {'finalized': '2023-05-09 03:31:25.277995+00:00', 'exit_code': 0})\n",
      "('lmeval-pythia-6.9b-step10000-cacb9eb3c6', {'finalized': '2023-05-09 03:32:03.977317+00:00', 'exit_code': 0})\n",
      "('lmeval-pythia-6.9b-step50000-715023ac71', {'finalized': '2023-05-09 03:31:10.948283+00:00', 'exit_code': 0})\n",
      "('lmeval-pythia-6.9b-step140000-a8eb0a3c7c', {'finalized': '2023-05-09 03:32:08.617660+00:00', 'exit_code': 0})\n",
      "('lmeval-pythia-160m-step1000-af94704947', {'finalized': '2023-05-09 02:26:21.667439+00:00', 'exit_code': 0})\n",
      "('lmeval-pythia-160m-step10000-3b698b190a', {'finalized': '2023-05-09 02:26:35.412889+00:00', 'exit_code': 0})\n",
      "('lmeval-pythia-160m-step50000-0784ce9ab1', {'finalized': '2023-05-09 02:23:52.417012+00:00', 'exit_code': 0})\n",
      "('lmeval-pythia-160m-step140000-18fc9c1c2f', {'finalized': '2023-05-09 02:23:52.654960+00:00', 'exit_code': 0})\n"
     ]
    }
   ],
   "source": [
    "for e in experiment_tracker:\n",
    "    print((e['exp_name'], get_experiment_status(e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a1be91-f117-4f9c-aec7-867b701107c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Save for later use (see cached version of this variable at end of notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7986304c-4cf0-4eae-8c66-a36e7e43e2a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/oyvindt/evals/olmo/beaker-result-cache/lmeval-experiments-2.jsonl'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_jsonl(os.path.join(CACHE_DIR, \"lmeval-experiments-2.jsonl\"), experiment_tracker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7af25-bc53-4319-a061-c680366ebbdc",
   "metadata": {},
   "source": [
    "## Running OLMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae94192-fc00-4eef-b719-edd9f1d97ab7",
   "metadata": {},
   "source": [
    "Running OLMo models is very similar, a few more arguments are needed in the command, and for now the model must be available as a beaker dataset (the actual model name doesn't matter here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4f2f7925-f1d7-47b4-92e2-5ccffcbc47f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TASK_SET = \"arc_challenge arc_easy boolq copa headqa_en hellaswag logiqa mathqa mrpc openbookqa piqa qnli qqp rte sciq sst wic winogrande wnli wsc\"\n",
    "OLMO_MODELS_TO_RUN = [ \n",
    "    {\"name\": \"olmo-c4-small-euox4j8q-step7300\", \"beaker_model\": \"oyvindt/olmo-c4-small-euox4j8q-step7300\", \"overrides\": {\"max_batch_tokens\": 2048*4}},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c9a48e64-582b-4dbf-9709-bf679d9d01c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_tracker_olmo = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8e707-4850-4f32-a937-903a2ac39fce",
   "metadata": {},
   "source": [
    "Launch the experiments, using this tweaked template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a849238e-0333-4f4f-9a11-401afb3e494e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GANTRY_TEMPLATE_ZEROSHOT_OLMO_1 = \"gantry run --gpus 1 --dataset '$$BEAKER_MODEL$$:/model' \\\n",
    "--venv base --workspace ai2/lm-eval \\\n",
    "--cluster ai2/aristo-cirrascale --beaker-image oyvindt/OLMoEvalV4 \\\n",
    "--env 'HF_DATASETS_CACHE=/net/nfs.cirrascale/aristo/oyvindt/hf_datasets_cache' $$GANTRY_ARGS$$ \\\n",
    "-- python catwalk/run_lm_eval.py --model $$MODEL$$ --task $$TASK$$ --split validation \\\n",
    "--full_output_file /results/predictions.jsonl --metrics_file /results/metrics.json --limit 1000 \\\n",
    "--num_recorded_inputs 3 --model_path /model --model_class olmo_eval.olmo_pretrained.OlmoPretrained \\\n",
    "--batch_size 32 --model_max_length 2048 --max_batch_tokens 20480\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ed9de3d2-8e9a-4bce-ab7f-0b30a6a3967f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running lmeval-olmo-c4-small-euox4j8q-step7300-30eca3ce7d\n"
     ]
    }
   ],
   "source": [
    "for model_spec in OLMO_MODELS_TO_RUN:\n",
    "    res = run_gantry_from_model_spec(model_spec, None, GANTRY_TEMPLATE_ZEROSHOT_OLMO_1, TASK_SET)\n",
    "    experiment_tracker_olmo.append(res)\n",
    "    time.sleep(0.5)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9c81c902-70c3-43a9-8880-45f2bbb75f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finalized': '2023-05-09 03:10:42.727976+00:00', 'exit_code': 0}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_experiment_status(experiment_tracker_olmo[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9a5e69fc-f250-4914-ac2e-7a99f892ec28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/oyvindt/evals/olmo/beaker-result-cache/lmeval-experiments-3.jsonl'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_jsonl(os.path.join(CACHE_DIR, \"lmeval-experiments-3.jsonl\"), experiment_tracker_olmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccaede1-9b78-456d-9223-2c244f383812",
   "metadata": {},
   "source": [
    "## Experiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb6811-3fc3-42e0-893c-615d75dd5cf9",
   "metadata": {},
   "source": [
    "First load experiment_tracker again, either from above or grab example output from bottom of this notebook to run on these example experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc3e26-685d-4f9b-84a3-56650b7a8f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_tracker = load_jsonl(os.path.join(CACHE_DIR, \"lmeval-experiments-1.jsonl\"))\n",
    "# experiment_tracker_olmo = load_jsonl(os.path.join(CACHE_DIR, \"lmeval-experiments-3.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d546ef3b-2b65-4800-a0dc-b59347f99e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiments = experiment_tracker + experiment_tracker_olmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba0b8d2-cb98-40d0-aedb-f5c47f5fbfb8",
   "metadata": {},
   "source": [
    "Download result files to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "d1e9079f-d9f9-4d2e-81aa-d5e7a537bded",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading results for lmeval-pythia-6.9b-step1000-9113d68b6a...\n",
      "Downloading results for lmeval-pythia-6.9b-step10000-cacb9eb3c6...\n",
      "Downloading results for lmeval-pythia-6.9b-step50000-715023ac71...\n",
      "Downloading results for lmeval-pythia-6.9b-step140000-a8eb0a3c7c...\n",
      "Downloading results for lmeval-pythia-160m-step1000-af94704947...\n",
      "Downloading results for lmeval-pythia-160m-step10000-3b698b190a...\n",
      "Downloading results for lmeval-pythia-160m-step50000-0784ce9ab1...\n",
      "Downloading results for lmeval-pythia-160m-step140000-18fc9c1c2f...\n",
      "Downloading results for lmeval-olmo-c4-small-euox4j8q-step7300-30eca3ce7d...\n"
     ]
    }
   ],
   "source": [
    "for e in experiments:\n",
    "    print(f\"Downloading results for {e['exp_name']}...\")\n",
    "    get_result_path(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401a8b9-822d-4cf1-8f55-07e0a16a45b9",
   "metadata": {},
   "source": [
    "Let's first look at the \"best\" pythia-6.9b-step14000 model a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c504d88f-3dab-4861-8598-e4a29afbaf9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment = experiments[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "280c8700-fa55-4981-a832-0a88e6308182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metrics.json', 'predictions.jsonl', '.gantry']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(get_result_path(experiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b1be8b3b-bb9c-4bcb-99d0-90c38e882b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = load_json(os.path.join(get_result_path(experiment), \"metrics.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f71201c-f698-49f2-94d7-314b0cfcccd7",
   "metadata": {},
   "source": [
    "Brief description of metrics:\n",
    "   * acc_raw: use total probability of completion\n",
    "   * acc_per_token: use probability per token\n",
    "   * acc_per_char: use probability per character (doesn't make much sense, but EleutherAI uses this for some tasks)\n",
    "   * acc_uncond: total probability of completion divided by \"unconditioned\" probability (completion prefixed only by something like \"Answer:\")\n",
    "   * predicted_indices_..: frequency of each predicted answer choice, by decreasing frequence (to catch cases where only one answer, like \"yes\", is almost always chosen)\n",
    "   * avg_total_probability_mass: total probability for all answer choices (averaged over instances)\n",
    "   * primary_metric: which of the scoring methods are used for the primary \"acc\" metric\n",
    "   * acc: primary accuracy, using one of the above methods (usually acc_per_token or acc_uncond\n",
    "   * total_token_count: total number of tokens across all instances\n",
    "   * max_token_count: max token count for a single input (e.g., to see if max model input length is hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b5c5637c-7626-4b59-a65c-50685850ea71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'arc_challenge',\n",
       " 'model': 'lm::pretrained=EleutherAI/pythia-6.9b,revision=step140000',\n",
       " 'task_options': {'unconditioned_prompt': 'Answer:',\n",
       "  'limit': 1000,\n",
       "  'split': 'validation',\n",
       "  'batch_size': 32,\n",
       "  'model_max_length': 2048,\n",
       "  'max_batch_tokens': 4096,\n",
       "  'num_recorded_inputs': 3},\n",
       " 'metrics': {'rc_metrics': {'acc_raw': 0.3177257525083612,\n",
       "   'predicted_indices_raw': [[0, 0.3612040133779264],\n",
       "    [1, 0.25418060200668896],\n",
       "    [3, 0.20735785953177258],\n",
       "    [2, 0.17725752508361203]],\n",
       "   'acc_per_token': 0.3612040133779264,\n",
       "   'predicted_indices_per_token': [[3, 0.2842809364548495],\n",
       "    [1, 0.25418060200668896],\n",
       "    [2, 0.23411371237458195],\n",
       "    [0, 0.22742474916387959]],\n",
       "   'acc_per_char': 0.36454849498327757,\n",
       "   'predicted_indices_per_char': [[3, 0.3210702341137124],\n",
       "    [2, 0.2408026755852843],\n",
       "    [0, 0.22408026755852842],\n",
       "    [1, 0.2140468227424749]],\n",
       "   'acc_uncond': 0.4414715719063545,\n",
       "   'predicted_indices_uncond': [[2, 0.3076923076923077],\n",
       "    [1, 0.2408026755852843],\n",
       "    [3, 0.2408026755852843],\n",
       "    [0, 0.21070234113712374]],\n",
       "   'avg_total_probability_mass': 0.0032092080231953916,\n",
       "   'primary_metric': 'acc_uncond',\n",
       "   'total_token_count': 44731,\n",
       "   'max_token_count': 134,\n",
       "   'acc': 0.4414715719063545}},\n",
       " 'num_instances': 299,\n",
       " 'processing_time_seconds': 48.84382486343384}"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics['metrics'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c398e-81a5-46ee-8a1e-1f6dabe67fdf",
   "metadata": {},
   "source": [
    "Main metric for each task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8858b62a-d822-44da-a676-15b2334e379e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arc_challenge: 0.441\n",
      "arc_easy: 0.619\n",
      "boolq: 0.405\n",
      "copa: 0.840\n",
      "headqa_en: 0.381\n",
      "hellaswag: 0.560\n",
      "logiqa: 0.215\n",
      "mathqa: 0.257\n",
      "mrpc: 0.654\n",
      "openbookqa: 0.450\n",
      "piqa: 0.762\n",
      "qnli: 0.515\n",
      "qqp: 0.608\n",
      "rte: 0.606\n",
      "sciq: 0.911\n",
      "sst: 0.623\n",
      "wic: 0.450\n",
      "winogrande: 0.595\n",
      "wnli: 0.620\n",
      "wsc: 0.635\n"
     ]
    }
   ],
   "source": [
    "for task in metrics['metrics']:\n",
    "    print(f\"{task['task']}: {task['metrics']['rc_metrics']['acc']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61162cdf-b008-4dd9-943b-0830bb6ff9df",
   "metadata": {},
   "source": [
    "Load full predictions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "660413f2-8f2a-4d26-a9b3-49b7ee411ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = load_jsonl(os.path.join(get_result_path(experiment), \"predictions.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "95679dd6-421d-4cc8-8e40-9369aa4695b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['task', 'model', 'task_options', 'metrics', 'num_instances', 'processing_time_seconds', 'per_instance'])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a4ad6fb3-7961-4735-8ce3-c59bfebf42de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instance': {'id': 'Mercury_SC_412337'},\n",
       " 'prediction': {'model_output': [{'sum_logits': -28.799480438232422,\n",
       "    'num_tokens': 14,\n",
       "    'num_chars': 69,\n",
       "    'num_tokens_all': 43,\n",
       "    'sum_logits_uncond': -43.893951416015625},\n",
       "   {'sum_logits': -31.447595596313477,\n",
       "    'num_tokens': 14,\n",
       "    'num_chars': 71,\n",
       "    'num_tokens_all': 43,\n",
       "    'sum_logits_uncond': -45.0609130859375},\n",
       "   {'sum_logits': -29.84939193725586,\n",
       "    'num_tokens': 14,\n",
       "    'num_chars': 69,\n",
       "    'num_tokens_all': 43,\n",
       "    'sum_logits_uncond': -45.9809455871582},\n",
       "   {'sum_logits': -32.15296936035156,\n",
       "    'num_tokens': 14,\n",
       "    'num_chars': 71,\n",
       "    'num_tokens_all': 43,\n",
       "    'sum_logits_uncond': -45.82860565185547}],\n",
       "  'correct_choice': 3,\n",
       "  'metrics': {'acc_raw': 0,\n",
       "   'predicted_index_raw': 0,\n",
       "   'acc_per_token': 0,\n",
       "   'predicted_index_per_token': 0,\n",
       "   'acc_per_char': 0,\n",
       "   'predicted_index_per_char': 0,\n",
       "   'acc_uncond': 0,\n",
       "   'predicted_index_uncond': 2,\n",
       "   'probability_mass': 4.525025555660871e-13,\n",
       "   'acc': 0}}}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]['per_instance'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef832b-39a0-467c-a66c-3fa03cb588b1",
   "metadata": {},
   "source": [
    "The first 3 (num_recorded_inputs) instances come with the full model inputs, the prompt and continuation for each answer choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e33300d4-f9a9-4a2d-b0a9-47688112af8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Question: Juan and LaKeisha roll a few objects down a ramp. They want to see which object rolls the farthest. What should they do so they can repeat their investigation?\\nAnswer:',\n",
       "  ' Put the objects in groups.'],\n",
       " ['Question: Juan and LaKeisha roll a few objects down a ramp. They want to see which object rolls the farthest. What should they do so they can repeat their investigation?\\nAnswer:',\n",
       "  ' Change the height of the ramp.'],\n",
       " ['Question: Juan and LaKeisha roll a few objects down a ramp. They want to see which object rolls the farthest. What should they do so they can repeat their investigation?\\nAnswer:',\n",
       "  ' Choose different objects to roll.'],\n",
       " ['Question: Juan and LaKeisha roll a few objects down a ramp. They want to see which object rolls the farthest. What should they do so they can repeat their investigation?\\nAnswer:',\n",
       "  ' Record the details of the investigation.']]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]['per_instance'][0]['model_input']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03837e-621c-41f3-a027-ca7a139ba27e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Not a pandas expert, but some hacky code to visualize a few things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d1e2e5c1-d36c-49f9-a8fa-ca52ddb0db3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "b3ffeb99-59e9-4975-9e31-fbb8878a15b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "by_task = {\"model\":[]}\n",
    "for experiment in experiments:\n",
    "    metrics1 = load_json(os.path.join(get_result_path(experiment), \"metrics.json\"))\n",
    "    by_task[\"model\"].append(experiment['model'].replace(\"EleutherAI/\",\"\").replace(\"-euox4j8q\",\"\")+\"-\"+experiment['checkpoint'])\n",
    "    for task in metrics1['metrics']:\n",
    "        task_name = task['task']\n",
    "        by_task[task_name] = by_task.get(task_name, []) + [task['metrics']['rc_metrics']['acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5f1d0c5f-87c1-4372-b8c5-d6cef7248f29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>arc_challenge</th>\n",
       "      <th>arc_easy</th>\n",
       "      <th>boolq</th>\n",
       "      <th>copa</th>\n",
       "      <th>headqa_en</th>\n",
       "      <th>hellaswag</th>\n",
       "      <th>logiqa</th>\n",
       "      <th>mathqa</th>\n",
       "      <th>mrpc</th>\n",
       "      <th>...</th>\n",
       "      <th>piqa</th>\n",
       "      <th>qnli</th>\n",
       "      <th>qqp</th>\n",
       "      <th>rte</th>\n",
       "      <th>sciq</th>\n",
       "      <th>sst</th>\n",
       "      <th>wic</th>\n",
       "      <th>winogrande</th>\n",
       "      <th>wnli</th>\n",
       "      <th>wsc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pythia-6.9b-step1000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.250877</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.211982</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.530686</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.470183</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.634615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pythia-6.9b-step10000</td>\n",
       "      <td>0.327759</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.213518</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.527076</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.591743</td>\n",
       "      <td>0.506270</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.634615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pythia-6.9b-step50000</td>\n",
       "      <td>0.361204</td>\n",
       "      <td>0.564912</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.201229</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.681373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.523466</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.676606</td>\n",
       "      <td>0.479624</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.634615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pythia-6.9b-step140000</td>\n",
       "      <td>0.441472</td>\n",
       "      <td>0.619298</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.654412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.606498</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.622706</td>\n",
       "      <td>0.449843</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.634615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pythia-160m-step1000</td>\n",
       "      <td>0.254181</td>\n",
       "      <td>0.285965</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.228879</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.527076</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.494266</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.634615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pythia-160m-step10000</td>\n",
       "      <td>0.250836</td>\n",
       "      <td>0.384211</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.201229</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.501805</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.511468</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.591549</td>\n",
       "      <td>0.634615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pythia-160m-step50000</td>\n",
       "      <td>0.257525</td>\n",
       "      <td>0.431579</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.196621</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.552347</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.510321</td>\n",
       "      <td>0.493730</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.634615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pythia-160m-step140000</td>\n",
       "      <td>0.284281</td>\n",
       "      <td>0.407018</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.205837</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.527076</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.509174</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.634615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>olmo-c4-small-step7300</td>\n",
       "      <td>0.287625</td>\n",
       "      <td>0.522807</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.184332</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.649510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.472924</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.557339</td>\n",
       "      <td>0.504702</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.557692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model  arc_challenge  arc_easy  boolq  copa  headqa_en   \n",
       "0    pythia-6.9b-step1000       0.230769  0.250877  0.414  0.54      0.266  \\\n",
       "1   pythia-6.9b-step10000       0.327759  0.447368  0.440  0.67      0.300   \n",
       "2   pythia-6.9b-step50000       0.361204  0.564912  0.373  0.72      0.346   \n",
       "3  pythia-6.9b-step140000       0.441472  0.619298  0.405  0.84      0.381   \n",
       "4    pythia-160m-step1000       0.254181  0.285965  0.391  0.51      0.240   \n",
       "5   pythia-160m-step10000       0.250836  0.384211  0.407  0.61      0.248   \n",
       "6   pythia-160m-step50000       0.257525  0.431579  0.387  0.66      0.276   \n",
       "7  pythia-160m-step140000       0.284281  0.407018  0.401  0.65      0.298   \n",
       "8  olmo-c4-small-step7300       0.287625  0.522807  0.456  0.77      0.291   \n",
       "\n",
       "   hellaswag    logiqa  mathqa      mrpc  ...   piqa   qnli    qqp       rte   \n",
       "0      0.281  0.211982   0.205  0.683824  ...  0.538  0.473  0.327  0.530686  \\\n",
       "1      0.420  0.213518   0.223  0.683824  ...  0.678  0.476  0.327  0.527076   \n",
       "2      0.506  0.201229   0.248  0.681373  ...  0.732  0.531  0.349  0.523466   \n",
       "3      0.560  0.215054   0.257  0.654412  ...  0.762  0.515  0.608  0.606498   \n",
       "4      0.300  0.228879   0.198  0.683824  ...  0.546  0.473  0.327  0.527076   \n",
       "5      0.345  0.201229   0.214  0.683824  ...  0.602  0.473  0.327  0.501805   \n",
       "6      0.358  0.196621   0.223  0.683824  ...  0.632  0.473  0.328  0.552347   \n",
       "7      0.358  0.205837   0.227  0.676471  ...  0.627  0.475  0.327  0.527076   \n",
       "8      0.506  0.184332   0.223  0.649510  ...  0.756  0.530  0.498  0.472924   \n",
       "\n",
       "    sciq       sst       wic  winogrande      wnli       wsc  \n",
       "0  0.424  0.470183  0.500000       0.516  0.563380  0.634615  \n",
       "1  0.835  0.591743  0.506270       0.495  0.563380  0.634615  \n",
       "2  0.887  0.676606  0.479624       0.590  0.422535  0.634615  \n",
       "3  0.911  0.622706  0.449843       0.595  0.619718  0.634615  \n",
       "4  0.452  0.494266  0.500000       0.510  0.563380  0.634615  \n",
       "5  0.758  0.511468  0.500000       0.526  0.591549  0.634615  \n",
       "6  0.788  0.510321  0.493730       0.521  0.563380  0.634615  \n",
       "7  0.744  0.509174  0.500000       0.512  0.563380  0.634615  \n",
       "8  0.824  0.557339  0.504702       0.582  0.521127  0.557692  \n",
       "\n",
       "[9 rows x 21 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame.from_dict(by_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b970ee42-95eb-4911-89c5-9b37e6530308",
   "metadata": {},
   "source": [
    "There's a lot more data available for analysis, such as catching \"degenerate\" tasks (always answer \"yes\") or compare different normalization approaches to the completions in ranked classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffd5f2-d9ca-4b0f-8208-eb24bda5472b",
   "metadata": {},
   "source": [
    "## Cached experiment_tracker and experiment_tracker_olmo variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb943e-1464-42d5-aab9-8a4ae0f7fef3",
   "metadata": {},
   "source": [
    "Full experiment_tracker and experiment_tracker_olmo variables so can go directly to analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "10a285dc-966b-4d58-8885-8884c7d13151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_tracker = [{'exp_name': 'lmeval-pythia-6.9b-step1000-9113d68b6a',\n",
    "  'model': 'EleutherAI/pythia-6.9b',\n",
    "  'checkpoint': 'step1000',\n",
    "  'run_gantry': {'command': \"gantry run --gpus 1 --venv base --workspace ai2/lm-eval --cluster ai2/aristo-cirrascale --beaker-image oyvindt/OLMoEvalV4 --env 'HF_DATASETS_CACHE=/net/nfs.cirrascale/aristo/oyvindt/hf_datasets_cache'  --name lmeval-pythia-6.9b-step1000-9113d68b6a -- python catwalk/run_lm_eval.py --model lm::pretrained=EleutherAI/pythia-6.9b,revision=step1000 --task arc_challenge arc_easy boolq copa headqa_en hellaswag logiqa mathqa mrpc openbookqa piqa qnli qqp rte sciq sst wic winogrande wnli wsc --split validation --full_output_file /results/predictions.jsonl --metrics_file /results/metrics.json --limit 1000 --num_recorded_inputs 3 --batch_size 32 --model_max_length 2048 --max_batch_tokens 4096\",\n",
    "   'output': '\\n\\x1b[1;36m                                             o=======[]   \\x1b[0m\\n\\x1b[1;36m   __ _                    _               _ |_      []   \\x1b[0m\\n\\x1b[1;36m  / _` |  __ _    _ _     | |_      _ _   | || |     []   \\x1b[0m\\n\\x1b[1;36m  \\\\__, | / _` |  | \\' \\\\    |  _|    | \\'_|   \\\\_, |   _/ ]_  \\x1b[0m\\n\\x1b[1;36m  |___/  \\\\__,_|  |_||_|   _\\\\__|   _|_|_   _|__/   |_____| \\x1b[0m\\n\\x1b[1;34m_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_| \"\"\"\"| \\x1b[0m\\n\\x1b[1;34m `---------------------------------------------\\' \\x1b[0m\\n\\nExperiment submitted, see progress at \\n\\x1b[4;94mhttps://beaker.org/ex/01GZZ6GKETZJWQSCXZ2S0N4RVM\\x1b[0m\\n'},\n",
    "  'id': '01GZZ6GKETZJWQSCXZ2S0N4RVM',\n",
    "  'result_id': '01GZZ6GNX72PWMHKERDP8EKJ9S'},\n",
    " {'exp_name': 'lmeval-pythia-6.9b-step10000-cacb9eb3c6',\n",
    "  'model': 'EleutherAI/pythia-6.9b',\n",
    "  'checkpoint': 'step10000',\n",
    "  'run_gantry': {'command': \"gantry run --gpus 1 --venv base --workspace ai2/lm-eval --cluster ai2/aristo-cirrascale --beaker-image oyvindt/OLMoEvalV4 --env 'HF_DATASETS_CACHE=/net/nfs.cirrascale/aristo/oyvindt/hf_datasets_cache'  --name lmeval-pythia-6.9b-step10000-cacb9eb3c6 -- python catwalk/run_lm_eval.py --model lm::pretrained=EleutherAI/pythia-6.9b,revision=step10000 --task arc_challenge arc_easy boolq copa headqa_en hellaswag logiqa mathqa mrpc openbookqa piqa qnli qqp rte sciq sst wic winogrande wnli wsc --split validation --full_output_file /results/predictions.jsonl --metrics_file /results/metrics.json --limit 1000 --num_recorded_inputs 3 --batch_size 32 --model_max_length 2048 --max_batch_tokens 4096\",\n",
    "   'output': '\\n\\x1b[1;36m                                             o=======[]   \\x1b[0m\\n\\x1b[1;36m   __ _                    _               _ |_      []   \\x1b[0m\\n\\x1b[1;36m  / _` |  __ _    _ _     | |_      _ _   | || |     []   \\x1b[0m\\n\\x1b[1;36m  \\\\__, | / _` |  | \\' \\\\    |  _|    | \\'_|   \\\\_, |   _/ ]_  \\x1b[0m\\n\\x1b[1;36m  |___/  \\\\__,_|  |_||_|   _\\\\__|   _|_|_   _|__/   |_____| \\x1b[0m\\n\\x1b[1;34m_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_| \"\"\"\"| \\x1b[0m\\n\\x1b[1;34m `---------------------------------------------\\' \\x1b[0m\\n\\nExperiment submitted, see progress at \\n\\x1b[4;94mhttps://beaker.org/ex/01GZZ6GPCVCSXXPZ02Z7RWS5J8\\x1b[0m\\n'},\n",
    "  'id': '01GZZ6GPCVCSXXPZ02Z7RWS5J8',\n",
    "  'result_id': '01GZZ6H7P43ZFN5KEZN0F1EWXR'},\n",
    " {'exp_name': 'lmeval-pythia-6.9b-step50000-715023ac71',\n",
    "  'model': 'EleutherAI/pythia-6.9b',\n",
    "  'checkpoint': 'step50000',\n",
    "  'run_gantry': {'command': \"gantry run --gpus 1 --venv base --workspace ai2/lm-eval --cluster ai2/aristo-cirrascale --beaker-image oyvindt/OLMoEvalV4 --env 'HF_DATASETS_CACHE=/net/nfs.cirrascale/aristo/oyvindt/hf_datasets_cache'  --name lmeval-pythia-6.9b-step50000-715023ac71 -- python catwalk/run_lm_eval.py --model lm::pretrained=EleutherAI/pythia-6.9b,revision=step50000 --task arc_challenge arc_easy boolq copa headqa_en hellaswag logiqa mathqa mrpc openbookqa piqa qnli qqp rte sciq sst wic winogrande wnli wsc --split validation --full_output_file /results/predictions.jsonl --metrics_file /results/metrics.json --limit 1000 --num_recorded_inputs 3 --batch_size 32 --model_max_length 2048 --max_batch_tokens 4096\",\n",
    "   'output': '\\n\\x1b[1;36m                                             o=======[]   \\x1b[0m\\n\\x1b[1;36m   __ _                    _               _ |_      []   \\x1b[0m\\n\\x1b[1;36m  / _` |  __ _    _ _     | |_      _ _   | || |     []   \\x1b[0m\\n\\x1b[1;36m  \\\\__, | / _` |  | \\' \\\\    |  _|    | \\'_|   \\\\_, |   _/ ]_  \\x1b[0m\\n\\x1b[1;36m  |___/  \\\\__,_|  |_||_|   _\\\\__|   _|_|_   _|__/   |_____| \\x1b[0m\\n\\x1b[1;34m_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_| \"\"\"\"| \\x1b[0m\\n\\x1b[1;34m `---------------------------------------------\\' \\x1b[0m\\n\\nExperiment submitted, see progress at \\n\\x1b[4;94mhttps://beaker.org/ex/01GZZ6GS83CJRRGCNY5SCR63D8\\x1b[0m\\n'},\n",
    "  'id': '01GZZ6GS83CJRRGCNY5SCR63D8',\n",
    "  'result_id': '01GZZ6H7TWJR2Y4NJDJQ7DWF5Z'},\n",
    " {'exp_name': 'lmeval-pythia-6.9b-step140000-a8eb0a3c7c',\n",
    "  'model': 'EleutherAI/pythia-6.9b',\n",
    "  'checkpoint': 'step140000',\n",
    "  'run_gantry': {'command': \"gantry run --gpus 1 --venv base --workspace ai2/lm-eval --cluster ai2/aristo-cirrascale --beaker-image oyvindt/OLMoEvalV4 --env 'HF_DATASETS_CACHE=/net/nfs.cirrascale/aristo/oyvindt/hf_datasets_cache'  --name lmeval-pythia-6.9b-step140000-a8eb0a3c7c -- python catwalk/run_lm_eval.py --model lm::pretrained=EleutherAI/pythia-6.9b,revision=step140000 --task arc_challenge arc_easy boolq copa headqa_en hellaswag logiqa mathqa mrpc openbookqa piqa qnli qqp rte sciq sst wic winogrande wnli wsc --split validation --full_output_file /results/predictions.jsonl --metrics_file /results/metrics.json --limit 1000 --num_recorded_inputs 3 --batch_size 32 --model_max_length 2048 --max_batch_tokens 4096\",\n",
    "   'output': '\\n\\x1b[1;36m                                             o=======[]   \\x1b[0m\\n\\x1b[1;36m   __ _                    _               _ |_      []   \\x1b[0m\\n\\x1b[1;36m  / _` |  __ _    _ _     | |_      _ _   | || |     []   \\x1b[0m\\n\\x1b[1;36m  \\\\__, | / _` |  | \\' \\\\    |  _|    | \\'_|   \\\\_, |   _/ ]_  \\x1b[0m\\n\\x1b[1;36m  |___/  \\\\__,_|  |_||_|   _\\\\__|   _|_|_   _|__/   |_____| \\x1b[0m\\n\\x1b[1;34m_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_| \"\"\"\"| \\x1b[0m\\n\\x1b[1;34m `---------------------------------------------\\' \\x1b[0m\\n\\nExperiment submitted, see progress at \\n\\x1b[4;94mhttps://beaker.org/ex/01GZZ6GW3CQQ3W2TY66MZNG6AY\\x1b[0m\\n'},\n",
    "  'id': '01GZZ6GW3CQQ3W2TY66MZNG6AY',\n",
    "  'result_id': '01GZZ6H7ZJD5S9TQE02E6JSAZN'},\n",
    " {'exp_name': 'lmeval-pythia-160m-step1000-af94704947',\n",
    "  'model': 'EleutherAI/pythia-160m',\n",
    "  'checkpoint': 'step1000',\n",
    "  'run_gantry': {'command': \"gantry run --gpus 1 --venv base --workspace ai2/lm-eval --cluster ai2/aristo-cirrascale --beaker-image oyvindt/OLMoEvalV4 --env 'HF_DATASETS_CACHE=/net/nfs.cirrascale/aristo/oyvindt/hf_datasets_cache'  --name lmeval-pythia-160m-step1000-af94704947 -- python catwalk/run_lm_eval.py --model lm::pretrained=EleutherAI/pythia-160m,revision=step1000 --task arc_challenge arc_easy boolq copa headqa_en hellaswag logiqa mathqa mrpc openbookqa piqa qnli qqp rte sciq sst wic winogrande wnli wsc --split validation --full_output_file /results/predictions.jsonl --metrics_file /results/metrics.json --limit 1000 --num_recorded_inputs 3 --batch_size 32 --model_max_length 2048 --max_batch_tokens 20480\",\n",
    "   'output': '\\n\\x1b[1;36m                                             o=======[]   \\x1b[0m\\n\\x1b[1;36m   __ _                    _               _ |_      []   \\x1b[0m\\n\\x1b[1;36m  / _` |  __ _    _ _     | |_      _ _   | || |     []   \\x1b[0m\\n\\x1b[1;36m  \\\\__, | / _` |  | \\' \\\\    |  _|    | \\'_|   \\\\_, |   _/ ]_  \\x1b[0m\\n\\x1b[1;36m  |___/  \\\\__,_|  |_||_|   _\\\\__|   _|_|_   _|__/   |_____| \\x1b[0m\\n\\x1b[1;34m_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_| \"\"\"\"| \\x1b[0m\\n\\x1b[1;34m `---------------------------------------------\\' \\x1b[0m\\n\\nExperiment submitted, see progress at \\n\\x1b[4;94mhttps://beaker.org/ex/01GZZ4YQ3CMVABGSJRN85BC00Q\\x1b[0m\\n'},\n",
    "  'id': '01GZZ4YQ3CMVABGSJRN85BC00Q',\n",
    "  'result_id': '01GZZ4Z76EBDX4K9BYB40CBDYK'},\n",
    " {'exp_name': 'lmeval-pythia-160m-step10000-3b698b190a',\n",
    "  'model': 'EleutherAI/pythia-160m',\n",
    "  'checkpoint': 'step10000',\n",
    "  'run_gantry': {'command': \"gantry run --gpus 1 --venv base --workspace ai2/lm-eval --cluster ai2/aristo-cirrascale --beaker-image oyvindt/OLMoEvalV4 --env 'HF_DATASETS_CACHE=/net/nfs.cirrascale/aristo/oyvindt/hf_datasets_cache'  --name lmeval-pythia-160m-step10000-3b698b190a -- python catwalk/run_lm_eval.py --model lm::pretrained=EleutherAI/pythia-160m,revision=step10000 --task arc_challenge arc_easy boolq copa headqa_en hellaswag logiqa mathqa mrpc openbookqa piqa qnli qqp rte sciq sst wic winogrande wnli wsc --split validation --full_output_file /results/predictions.jsonl --metrics_file /results/metrics.json --limit 1000 --num_recorded_inputs 3 --batch_size 32 --model_max_length 2048 --max_batch_tokens 20480\",\n",
    "   'output': '\\n\\x1b[1;36m                                             o=======[]   \\x1b[0m\\n\\x1b[1;36m   __ _                    _               _ |_      []   \\x1b[0m\\n\\x1b[1;36m  / _` |  __ _    _ _     | |_      _ _   | || |     []   \\x1b[0m\\n\\x1b[1;36m  \\\\__, | / _` |  | \\' \\\\    |  _|    | \\'_|   \\\\_, |   _/ ]_  \\x1b[0m\\n\\x1b[1;36m  |___/  \\\\__,_|  |_||_|   _\\\\__|   _|_|_   _|__/   |_____| \\x1b[0m\\n\\x1b[1;34m_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_| \"\"\"\"| \\x1b[0m\\n\\x1b[1;34m `---------------------------------------------\\' \\x1b[0m\\n\\nExperiment submitted, see progress at \\n\\x1b[4;94mhttps://beaker.org/ex/01GZZ4YRTK4EV277WCT6QQBB64\\x1b[0m\\n'},\n",
    "  'id': '01GZZ4YRTK4EV277WCT6QQBB64',\n",
    "  'result_id': '01GZZ4Z7B2V7NMM81RHGHD0WYH'},\n",
    " {'exp_name': 'lmeval-pythia-160m-step50000-0784ce9ab1',\n",
    "  'model': 'EleutherAI/pythia-160m',\n",
    "  'checkpoint': 'step50000',\n",
    "  'run_gantry': {'command': \"gantry run --gpus 1 --venv base --workspace ai2/lm-eval --cluster ai2/aristo-cirrascale --beaker-image oyvindt/OLMoEvalV4 --env 'HF_DATASETS_CACHE=/net/nfs.cirrascale/aristo/oyvindt/hf_datasets_cache'  --name lmeval-pythia-160m-step50000-0784ce9ab1 -- python catwalk/run_lm_eval.py --model lm::pretrained=EleutherAI/pythia-160m,revision=step50000 --task arc_challenge arc_easy boolq copa headqa_en hellaswag logiqa mathqa mrpc openbookqa piqa qnli qqp rte sciq sst wic winogrande wnli wsc --split validation --full_output_file /results/predictions.jsonl --metrics_file /results/metrics.json --limit 1000 --num_recorded_inputs 3 --batch_size 32 --model_max_length 2048 --max_batch_tokens 20480\",\n",
    "   'output': '\\n\\x1b[1;36m                                             o=======[]   \\x1b[0m\\n\\x1b[1;36m   __ _                    _               _ |_      []   \\x1b[0m\\n\\x1b[1;36m  / _` |  __ _    _ _     | |_      _ _   | || |     []   \\x1b[0m\\n\\x1b[1;36m  \\\\__, | / _` |  | \\' \\\\    |  _|    | \\'_|   \\\\_, |   _/ ]_  \\x1b[0m\\n\\x1b[1;36m  |___/  \\\\__,_|  |_||_|   _\\\\__|   _|_|_   _|__/   |_____| \\x1b[0m\\n\\x1b[1;34m_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_| \"\"\"\"| \\x1b[0m\\n\\x1b[1;34m `---------------------------------------------\\' \\x1b[0m\\n\\nExperiment submitted, see progress at \\n\\x1b[4;94mhttps://beaker.org/ex/01GZZ4YTHWCBAZVJNYG042BG7M\\x1b[0m\\n'},\n",
    "  'id': '01GZZ4YTHWCBAZVJNYG042BG7M',\n",
    "  'result_id': '01GZZ4Z7FKJBRHHF8NW5FNFJDX'},\n",
    " {'exp_name': 'lmeval-pythia-160m-step140000-18fc9c1c2f',\n",
    "  'model': 'EleutherAI/pythia-160m',\n",
    "  'checkpoint': 'step140000',\n",
    "  'run_gantry': {'command': \"gantry run --gpus 1 --venv base --workspace ai2/lm-eval --cluster ai2/aristo-cirrascale --beaker-image oyvindt/OLMoEvalV4 --env 'HF_DATASETS_CACHE=/net/nfs.cirrascale/aristo/oyvindt/hf_datasets_cache'  --name lmeval-pythia-160m-step140000-18fc9c1c2f -- python catwalk/run_lm_eval.py --model lm::pretrained=EleutherAI/pythia-160m,revision=step140000 --task arc_challenge arc_easy boolq copa headqa_en hellaswag logiqa mathqa mrpc openbookqa piqa qnli qqp rte sciq sst wic winogrande wnli wsc --split validation --full_output_file /results/predictions.jsonl --metrics_file /results/metrics.json --limit 1000 --num_recorded_inputs 3 --batch_size 32 --model_max_length 2048 --max_batch_tokens 20480\",\n",
    "   'output': '\\n\\x1b[1;36m                                             o=======[]   \\x1b[0m\\n\\x1b[1;36m   __ _                    _               _ |_      []   \\x1b[0m\\n\\x1b[1;36m  / _` |  __ _    _ _     | |_      _ _   | || |     []   \\x1b[0m\\n\\x1b[1;36m  \\\\__, | / _` |  | \\' \\\\    |  _|    | \\'_|   \\\\_, |   _/ ]_  \\x1b[0m\\n\\x1b[1;36m  |___/  \\\\__,_|  |_||_|   _\\\\__|   _|_|_   _|__/   |_____| \\x1b[0m\\n\\x1b[1;34m_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_| \"\"\"\"| \\x1b[0m\\n\\x1b[1;34m `---------------------------------------------\\' \\x1b[0m\\n\\nExperiment submitted, see progress at \\n\\x1b[4;94mhttps://beaker.org/ex/01GZZ4YW7S555PBRH31SJ463A3\\x1b[0m\\n'},\n",
    "  'id': '01GZZ4YW7S555PBRH31SJ463A3',\n",
    "  'result_id': '01GZZ4Z7M4G263SK6GZN7K1AF8'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "05e94914-dbe7-43ad-be1f-e394a4a09b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_tracker_olmo = [{'exp_name': 'lmeval-olmo-c4-small-euox4j8q-step7300-30eca3ce7d',\n",
    "  'model': 'olmo-c4-small-euox4j8q-step7300',\n",
    "  'checkpoint': None,\n",
    "  'run_gantry': {'command': \"gantry run --gpus 1 --dataset 'oyvindt/olmo-c4-small-euox4j8q-step7300:/model' --venv base --workspace ai2/lm-eval --cluster ai2/aristo-cirrascale --beaker-image oyvindt/OLMoEvalV4 --env 'HF_DATASETS_CACHE=/net/nfs.cirrascale/aristo/oyvindt/hf_datasets_cache'  --name lmeval-olmo-c4-small-euox4j8q-step7300-30eca3ce7d -- python catwalk/run_lm_eval.py --model lm::pretrained=olmo-c4-small-euox4j8q-step7300 --task arc_challenge arc_easy boolq copa headqa_en hellaswag logiqa mathqa mrpc openbookqa piqa qnli qqp rte sciq sst wic winogrande wnli wsc --split validation --full_output_file /results/predictions.jsonl --metrics_file /results/metrics.json --limit 1000 --num_recorded_inputs 3 --model_path /model --model_class olmo_eval.olmo_pretrained.OlmoPretrained --batch_size 32 --model_max_length 2048 --max_batch_tokens 8192\",\n",
    "   'output': '\\n\\x1b[1;36m                                             o=======[]   \\x1b[0m\\n\\x1b[1;36m   __ _                    _               _ |_      []   \\x1b[0m\\n\\x1b[1;36m  / _` |  __ _    _ _     | |_      _ _   | || |     []   \\x1b[0m\\n\\x1b[1;36m  \\\\__, | / _` |  | \\' \\\\    |  _|    | \\'_|   \\\\_, |   _/ ]_  \\x1b[0m\\n\\x1b[1;36m  |___/  \\\\__,_|  |_||_|   _\\\\__|   _|_|_   _|__/   |_____| \\x1b[0m\\n\\x1b[1;34m_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_|\"\"\"\"\"|_| \"\"\"\"| \\x1b[0m\\n\\x1b[1;34m `---------------------------------------------\\' \\x1b[0m\\n\\nExperiment submitted, see progress at \\n\\x1b[4;94mhttps://beaker.org/ex/01GZZ75J196QP1JEYTPG0YEEB9\\x1b[0m\\n'},\n",
    "  'id': '01GZZ75J196QP1JEYTPG0YEEB9',\n",
    "  'result_id': '01GZZ760XEJ6AXZEKFM1B9TT98'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5fab20-b504-493a-afad-5575bf3d4d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catwalk",
   "language": "python",
   "name": "catwalk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
