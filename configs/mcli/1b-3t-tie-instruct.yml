name: 1b-3t-tie-instruct
image: mosaicml/pytorch:2.1.0_cu121-python3.10-ubuntu20.04
env_variables:
  CONFIG_PATH: "configs/v1_5-mix-small/1b-3t-tie-instruct.yml"
  LOAD_PATH: "s3://ai2-llm/checkpoints/olmo-small-3T-lower-lr-tie/5683801/step738020-unsharded"
  # LEARNING_RATE: 1e-5
  # WARMUP_STEPS: 2000
  # MAX_EPOCHS: 10
  LEARNING_RATE: 1e-5
  WARMUP_STEPS: 400
  MAX_EPOCHS: 10
compute:
  gpus: 32
  cluster: r12z3
  gpu_type: a100_40gb
integrations:
  - integration_type: git_repo
    git_repo: allenai/OLMo
    git_branch: soldni/3T-1B-D15
    git_commit: a3f4ed8ab5164fc81ea1e72ac9a1416d4be39fad
    pip_install: -e .
    ssh_clone: true
command: |-
  # Install aws cli
  apt-get update
  apt-get install zip unzip
  curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
  unzip awscliv2.zip
  sudo ./aws/install

  cd OLMo
  pip freeze
  mkdir -p /root/.cache/torch/

  export OMP_NUM_THREADS=8
  export LOG_FILTER_TYPE=local_rank0_only

  # get the batch size from number of gpus
  BATCH_SIZE=$((${NUM_NODES} * 4))

  # get run name, we will use this as run name in mcli
  RUN_NAME=$(cat $CONFIG_PATH | grep -ohP "^run_name\:\w*(.+)$" | sed 's/run_name:\s*//')

  # get a hash of the load path and config path; take the first 8 characters
  RUN_HASH=$(echo "${LOAD_PATH_ARG}-${CONFIG_PATH}" | md5sum | cut -c 1-8)

  # compose full run name
  FULL_RUN_NAME="${RUN_NAME}-${LEARNING_RATE}-${WARMUP_STEPS}-${MAX_EPOCHS}ep-${BATCH_SIZE}bs-${RUN_HASH}"

  # get W&B settings from the config file, then extract the project and group
  WANDB_SETTINGS=$(cat $CONFIG_PATH |  tr '\n' '\r' | grep -ohP "\rwandb:\r.*?\r\r"  | tr '\r' '\n')
  export WANDB_PROJECT=$(echo $WANDB_SETTINGS | grep -ohP "\w*project\:\s*\S+(\s|$)" | sed 's/project:\s*//')
  if [[ -z "${WANDB_PROJECT}" ]]; then
    export WANDB_PROJECT="olmo-small"
  fi

  # check if W&B is provided; if not, use the run name as the project name
  # (the actual run rame with have slurm ID appended)
  export WANDB_GROUP=$(echo $WANDB_SETTINGS | grep -ohP "\w*group\:\w*(.+)" | sed 's/group:\s*//')
  if [[ -z "${WANDB_GROUP}" ]]; then
    export WANDB_GROUP="${RUN_NAME}"
  fi

  # save path
  SAVE_FOLDER="/runs/${FULL_RUN_NAME}"
  mkdir -p "${SAVE_FOLDER}"

  # Even on mosaic, set a time limit of 48 hours for now
  TIME_LIMIT=172800

  # If a checkpoint is provided, download it
  if [[ -z "${LOAD_PATH}" ]]; then
    LOAD_PATH_ARG=""
  else
    echo "Downloading checkpoint from ${LOAD_PATH}"
    mkdir -p /root/checkpoint-unsharded
    aws s3 sync "${LOAD_PATH}" /root/checkpoint-unsharded
    LOAD_PATH_ARG="--load_path=/root/checkpoint-unsharded"
    echo "Checkpoint downloaded."
  fi


  CUDA_LAUNCH_BLOCKING=1 torchrun \
  --master_addr "$MASTER_ADDR" \
  --master_port "$MASTER_PORT" \
  --nnodes "$NUM_NODES" \
  --node_rank "$NODE_RANK" \
  --nproc_per_node 8 \
  scripts/train.py \
    ${CONFIG_PATH} \
    --run_name="${FULL_RUN_NAME}" \
    --wandb.project="${WANDB_PROJECT}" \
    --wandb.group="${WANDB_GROUP}" \
    --optimizer.learning_rate=${LEARNING_RATE} \
    --scheduler.grad_clip_warmup_steps=${WARMUP_STEPS} \
    --scheduler.t_warmup="${WARMUP_STEPS}" \
    --save_overwrite \
    --save_interval_unsharded=1000 \
    --global_train_batch_size=${BATCH_SIZE} \
    $LOAD_PATH_ARG \
    --reset_trainer_state \
    --reset_optimizer_state \
    --max_duration="${MAX_EPOCHS}ep"
